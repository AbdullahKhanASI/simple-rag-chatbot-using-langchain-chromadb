
# PRD â€” Simple RAG Chatbot (CLI) with LangChain & ChromaDB

---

## 1. Purpose
Build a lightweight Retrievalâ€‘Augmented Generation (RAG) chatbot that:
1. **Indexes** all PDF documents located in a `docs/` subâ€‘folder.
2. **Stores** embeddings and metadata in a local ChromaDB instance.
3. **Answers** user questions from the terminal by retrieving relevant chunks and generating concise responses via an LLM (e.g., OpenAI GPTâ€‘4o).

---

## 2. Goals & SuccessÂ Metrics
| Goal | Metric | Target |
|------|--------|--------|
| Accurate retrieval | Topâ€‘k retrieved chunks are relevant | â‰¥Â 90% manual relevance on test set |
| Fast response | Endâ€‘toâ€‘end latency per query | â‰¤Â 3â€¯s on laptop CPU |
| Easy setup | Oneâ€‘command install & run | `make setup` then `python chat.py` |

---

## 3. Background
Many teams want an offline, scriptable RAG reference. This project demonstrates a **minimal twoâ€‘file** patternâ€”one ingestion script, one chat scriptâ€”using the LangChain and ChromaDB ecosystem.

---

## 4. Scope
### 4.1 In Scope
* Local ingestion of PDFs in `docs/`
* Embedding with `langchain.embeddings.OpenAIEmbeddings` (pluggable)
* Storage in **ChromaDB** (persistent, local)
* CLI chat with streaming answers
* Basic error handling & logging

### 4.2 Out of Scope
* Web UI, API server, or multiâ€‘user auth
* Nonâ€‘PDF formats (can be future work)
* Cloud deployment / vector DB as a service

---

## 5. FunctionalÂ Requirements
| ID | Requirement |
|----|-------------|
| **FRâ€‘1** | The system **ingests** every PDF in `docs/` at startup or on demand. |
| **FRâ€‘2** | Embeddings are generated in batches (configurable batch size). |
| **FRâ€‘3** | Metadata includesÂ `source_path`, `page_number`, and `chunk_id`. |
| **FRâ€‘4** | The **chat** script loads the same ChromaDB collection. |
| **FRâ€‘5** | For each user query, topâ€‘k (defaultÂ =Â 4) chunks are retrieved. |
| **FRâ€‘6** | Retrieved chunks are passed to an LLM via LangChain `ConversationalRetrievalChain`. |
| **FRâ€‘7** | The terminal prints the answer and, optionally, citations. |

---

## 6. Nonâ€‘FunctionalÂ Requirements
* **Performance:** â‰¤Â 3â€¯s latency on an M1 MacBook @ 4Â chunks, GPTâ€‘4o.
* **Extensibility:** Swap embedding/LLM classes via `.env` or CLI flags.
* **Reliability:** Graceful failure if PDFs are missing or ChromaDB is corrupt.
* **Security:** No external network calls beyond the LLM provider.

---

## 7. UserÂ Stories
1. **As a user**, I want to run `python ingest.py` so that my PDFs are indexed.
2. **As a user**, I want to ask â€œ_What is the refund policy?_â€ and get an answer sourced from my PDFs.
3. **As a maintainer**, I want clear logging so I can debug ingestion failures.

---

## 8. Technical Design

### 8.1 Architecture (Textual)
`PDFs â†’ LangChainÂ loaders (PyPDFLoader) â†’ Splitter (RecursiveCharacterTextSplitter) â†’ Embeddings â†’ ChromaDB (persistent) â† Retriever â† ConversationalRetrievalChain â† LLM (GPTâ€‘4o)`

### 8.2 File & Folder Structure
```
rag-chatbot/
â”œâ”€â”€ docs/               # PDF knowledge base
â”œâ”€â”€ ingest.py           # Ingest & index PDFs
â”œâ”€â”€ chat.py             # CLI chat interface
â”œâ”€â”€ .env.example        # API keys & config
â””â”€â”€ requirements.txt
```

### 8.3 Key Components
| File | Responsibility |
|------|----------------|
| `ingest.py` | Scan `docs/`, split pages into chunks, embed, and upsert into ChromaDB collection `pdf_knowledge`. |
| `chat.py` | Load collection, build retrievalâ€‘augmented chain, prompt user for queries in REPL loop, stream answers. |

#### `ingest.py` Highâ€‘Level Flow
1. Load `.env` (API keys, chunk size, batch size, persist_directory).
2. Initialize `Chroma` with `persist_directory="db"` and `collection_name="pdf_knowledge"`.
3. For each PDF:
   * Use `PyPDFLoader` â†’ `load_and_split`.
   * Add `metadata={"source": path, "page": page_no}`.
4. Embed with OpenAI or HuggingFace embeddings.
5. `collection.add_texts(texts, metadatas)`.
6. PersistÂ DB (`client.persist()`).

#### `chat.py` Highâ€‘Level Flow
1. Load `.env`.
2. Instantiate retriever: `client.get_collection("pdf_knowledge").as_retriever(search_type="similarity", k=4)`.
3. Wrap in `ConversationalRetrievalChain` (memory optional).
4. Start REPL:

```python
while True:
    query = input("â“  You: ")
    if query.lower() in {"exit", "quit"}:
        break
    result = qa({"question": query, "chat_history": chat_history})
    print("ğŸ¤– RAGbot:", result["answer"])
    chat_history.append((query, result["answer"]))
```

### 8.4 Environment Variables
| Variable | Description |
|----------|-------------|
| `OPENAI_API_KEY` | Key for GPTâ€‘4o or chosen model. |
| `EMBEDDING_MODEL` | e.g., `text-embedding-3-small`. |
| `PERSIST_DIR` | Folder for Chroma files (default: `db`). |

### 8.5 Installation & Setup
```bash
git clone https://github.com/yourâ€‘org/rag-chatbot.git
cd rag-chatbot
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
cp .env.example .env  # add your keys
python ingest.py     # index PDFs
python chat.py       # start chatting
```

---

## 9. AcceptanceÂ Criteria
* ACâ€‘1: Running `python ingest.py` with 5 sample PDFs completes without errors.
* ACâ€‘2: `chat.py` returns an answer containing at least one citation token like â€œ(source.pdf, p.â€¯2)â€.
* ACâ€‘3: Latency â‰¤Â 3â€¯s for queries under 100 tokens.

---

## 10. Timeline (Tentative)
| Milestone | Deliverable | Owner | ETA |
|-----------|-------------|-------|-----|
| M1 | Repo scaffolding & env setup | Dev | DayÂ 1 |
| M2 | `ingest.py` functional | Dev | DayÂ 2 |
| M3 | `chat.py` functional | Dev | DayÂ 3 |
| M4 | README & sample PDFs | Dev | DayÂ 3 |
| M5 | Testing & polish | Dev | DayÂ 4 |

---

## 11. Risks & Mitigations
| Risk | Impact | Mitigation |
|------|--------|-----------|
| OpenAI API quota limits | High | Allow HF embeddings / LLM fallback; caching. |
| Large PDFs exceed token limits | Medium | Recursive splitter, chunk size tuning. |
| Nonâ€‘PDF files present | Low | Filter by `.pdf` extension. |

---

## 12. FutureÂ Enhancements
* Add web UI (Streamlit, Next.js).
* Support additional file types (Markdown, HTML).
* Switch to managed vector DB (Pinecone, Weaviate).
* Multiâ€‘user conversation context with auth.

---
